{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoading (mypath):\n",
    "    print (\"Loading the data\")\n",
    "    dataframe = pd.read_csv(mypath,header = None,engine = 'python',sep=\",\")\n",
    "    return dataframe\n",
    "\n",
    "def DataPreprocessing(mydataframe):\n",
    "    \n",
    "    # Dropping the duplicates\n",
    "    recordcount = len(mydataframe)\n",
    "    print (\"Original number of records in the training dataset before removing duplicates is: \" , recordcount)\n",
    "    mydataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "    newrecordcount = len(mydataframe)\n",
    "    print (\"Number of records in the training dataset after removing the duplicates is :\", newrecordcount,\"\\n\")\n",
    "\n",
    "    #Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "    df_X = mydataframe.drop(mydataframe.columns[41],axis=1,inplace = False)\n",
    "    df_Y = mydataframe.drop(mydataframe.columns[0:41],axis=1, inplace = False)\n",
    "\n",
    "    # Convert Categorial data to the numerical data for the efficient classification\n",
    "    df_X[df_X.columns[1:4]] = df_X[df_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "    \n",
    "    # Coding the normal as \" 1 0\" and attack as \"0 1\"\n",
    "    df_Y[df_Y[41]!='normal.'] = 0\n",
    "    df_Y[df_Y[41]=='normal.'] = 1\n",
    "    #print (labels[41].value_counts())\n",
    "    \n",
    "    #converting input data into float which is requried in the future stage of building in the network\n",
    "    df_X = df_X.loc[:,df_X.columns[0:41]].astype(float)\n",
    "\n",
    "    # Normal is \"1 0\" and the abnormal is \"0 1\"\n",
    "    df_Y.columns = [\"y1\"]\n",
    "    df_Y.loc[:,('y2')] = df_Y['y1'] ==0\n",
    "    df_Y.loc[:,('y2')] = df_Y['y2'].astype(int)\n",
    "    \n",
    "\n",
    "    \n",
    "    return df_X,df_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureSelection(myinputX, myinputY):\n",
    "\n",
    "    labels = np.array(myinputY).astype(int)\n",
    "    inputX = np.array(myinputX)\n",
    "    \n",
    "    #Random Forest Model\n",
    "    model = RandomForestClassifier(random_state = 0)\n",
    "    model.fit(inputX,labels)\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    \n",
    "    #Plotting the Features agains their importance scores\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.title(\"Feature importances (y-axis) vs Features IDs(x-axis)\")\n",
    "    plt.bar(range(inputX.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(inputX.shape[1]), indices)\n",
    "    plt.xlim([-1, inputX.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "    # Selecting top featueres which have higher importance values = here we can find \"12\" features\n",
    "    #as we can see in the next step\n",
    "    newX = myinputX.iloc[:,model.feature_importances_.argsort()[::-1][:12]]\n",
    "   # Converting the X dataframe into tensors\n",
    "    myX = newX.as_matrix()\n",
    "    myY = labels\n",
    "\n",
    "   \n",
    "    return myX,myY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Laoding the IDS Data\")\n",
    "data_path = \"Macintosh HD:\\\\Users\\aimable\\downloads\\IoT\\Final\\DataPrep\\Results\\FinalApp.txt\"\n",
    "dataframe = DataLoading(data_path)\n",
    "\n",
    "print (\"Data Preprocessing of loaded IDS Data\")\n",
    "data_X, data_Y = DataPreprocessing(dataframe)\n",
    "\n",
    "print (\"Performing the Feature Selection on train data set\")\n",
    "reduced_X,reduced_Y = FeatureSelection(data_X,data_Y)\n",
    "\n",
    "#Dividing the dataset into train and test datasets\n",
    "# Out of 13051 samples = 80% samples as train data and 20% samples as test data\n",
    "\n",
    "# Train features and Train Labels\n",
    "train_X = reduced_X[:8000]\n",
    "train_Y = reduced_Y[:8000]\n",
    "\n",
    "#Test Features and Test Labels\n",
    "test_X = reduced_X[8001:10000]\n",
    "test_Y = reduced_Y[8001:10000]\n",
    "\n",
    "\n",
    "print (\"Train X shape is :\", train_X.shape)\n",
    "print (\"Train Y shape is :\", train_Y.shape)\n",
    "print (\"Test X shape is :\", test_X.shape)\n",
    "print (\"Test Y shape is :\", test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before normalizing, the array of input features should be converted to a dataframe\n",
    "semitrain_X = pd.DataFrame(train_X)\n",
    "semitest_X = pd.DataFrame(test_X)\n",
    "#Importing Scikit learn libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#Normalizing Train Data Features\n",
    "scaler_traindata = scaler.fit(semitrain_X)\n",
    "train_norm = scaler_traindata.transform(semitrain_X)\n",
    "train_norm_X = pd.DataFrame(train_norm)\n",
    "\n",
    "#Normalizing Test Data Features\n",
    "scaler_testdata = scaler.fit(semitest_X)\n",
    "test_norm = scaler_testdata.transform(semitest_X)\n",
    "test_norm_X = pd.DataFrame(test_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Hyper Parameters for the model\n",
    "learning_rate = 0.001\n",
    "n_classes = 2\n",
    "display_step = 100\n",
    "input_features = train_X.shape[1] #No of selected features(columns)\n",
    "training_cycles = 1000\n",
    "time_steps = 5 # No of time-steps to backpropogate\n",
    "hidden_units = 50 #No of LSTM units in a LSTM Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float64,shape = [None,time_steps,input_features], name = \"x-input\")\n",
    "    y = tf.placeholder(tf.float64, shape = [None,n_classes],name = \"y-input\")\n",
    "#Weights and Biases\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.random_normal([hidden_units,n_classes]),name = \"layer-weights\")\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.random_normal([n_classes]),name = \"unit-biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify data with respect to the time steps count\n",
    "def rnn_data(data, time_steps, labels=False):\n",
    "    \"\"\"\n",
    "    creates new data frame based on previous observation\n",
    "      * example:\n",
    "        l = [1, 2, 3, 4, 5]\n",
    "        time_steps = 2\n",
    "        -> labels == False [[1, 2], [2, 3], [3, 4]]\n",
    "        -> labels == True [3, 4, 5]\n",
    "    \"\"\"\n",
    "    rnn_df = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        if labels:\n",
    "            try:\n",
    "                rnn_df.append(data.iloc[i + time_steps].as_matrix())\n",
    "            except AttributeError:\n",
    "                rnn_df.append(data.iloc[i + time_steps])\n",
    "        else:\n",
    "            data_ = data.iloc[i: i + time_steps].as_matrix()\n",
    "            rnn_df.append(data_ if len(data_.shape) > 1 else [[i] for i in data_])\n",
    "\n",
    "    return np.array(rnn_df, dtype=np.float32)\n",
    "\n",
    "# Modifications to train data\n",
    "train_data_X = pd.DataFrame(train_norm_X)\n",
    "train_data_Y = pd.DataFrame(train_Y)\n",
    "newtrain_X = rnn_data(train_data_X,time_steps,labels = False)\n",
    "newtrain_Y = rnn_data(train_data_Y,time_steps,labels = True)\n",
    "\n",
    "print (\"Shape of new train X\",newtrain_X.shape)\n",
    "print (\"Shape of new train Y\",newtrain_Y.shape)\n",
    "\n",
    "# Modifications to test data\n",
    "test_data_X = pd.DataFrame(test_norm_X)\n",
    "test_data_Y = pd.DataFrame(test_Y)\n",
    "newtest_X = rnn_data(test_data_X,time_steps,labels = False)\n",
    "newtest_Y = rnn_data(test_data_Y,time_steps,labels = True)\n",
    "\n",
    "print (\"Shape of new test X\",newtest_X.shape)\n",
    "print (\"Shape of new test Y\",newtest_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unstacking the inputs with time steps to provide the inputs in sequence\n",
    "# Unstack to get a list of 'time_steps' tensors of shape (batch_size, input_features)\n",
    "x_ = tf.unstack(x,time_steps,axis =1)\n",
    "\n",
    "#Defining a single GRU cell\n",
    "gru_cell = tf.contrib.rnn.GRUCell(hidden_units)\n",
    "\n",
    "#GRU Output\n",
    "with tf.variable_scope('MyGRUCel36'):\n",
    "    gruoutputs,grustates = tf.contrib.rnn.static_rnn(gru_cell,x_,dtype=tf.float64)\n",
    "    \n",
    "#Linear Activation , using gru inner loop last output\n",
    "output =  tf.add(tf.matmul(gruoutputs[-1],tf.cast(W,tf.float64)),tf.cast(b,tf.float64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits = output))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the Model\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range (training_cycles):\n",
    "    _,c = sess.run([optimizer,cost], feed_dict = {x:newtrain_X, y:newtrain_Y})\n",
    "    \n",
    "    if (i) % display_step == 0:\n",
    "        print (\"Cost for the training cycle : \",i,\" : is : \",sess.run(cost, feed_dict ={x :newtrain_X,y:newtrain_Y}))\n",
    "correct = tf.equal(tf.argmax(output, 1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "print('Accuracy on the overall test set is :',accuracy.eval({x:newtest_X, y:newtest_Y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.argmax = Returns the index with the largest value across axes of a tensor.\n",
    "# Therefore, we are extracting the final labels => '1 0' = '1' = Normal (and vice versa)\n",
    "# Steps to calculate the confusion matrix\n",
    "\n",
    "pred_class = sess.run(tf.argmax(output,1),feed_dict = {x:newtest_X,y:newtest_Y})\n",
    "labels_class = sess.run(tf.argmax(y,1),feed_dict = {x:newtest_X,y:newtest_Y})\n",
    "conf = tf.contrib.metrics.confusion_matrix(labels_class,pred_class,dtype = tf.int32)\n",
    "ConfM = sess.run(conf, feed_dict={x:newtest_X,y:newtest_Y})\n",
    "print (\"confusion matrix \\n\",ConfM)\n",
    "\n",
    "#Plotting the Confusion Matrix\n",
    "labels = ['Normal', 'Attack']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(ConfM)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.argmax = Returns the index with the largest value across axes of a tensor.\n",
    "# Therefore, we are extracting the final labels => '1 0' = '1' = Normal (and vice versa)\n",
    "# Steps to calculate the confusion matrix\n",
    "\n",
    "pred_class = sess.run(tf.argmax(output,1),feed_dict = {x:newtest_X,y:newtest_Y})\n",
    "labels_class = sess.run(tf.argmax(y,1),feed_dict = {x:newtest_X,y:newtest_Y})\n",
    "conf = tf.contrib.metrics.confusion_matrix(labels_class,pred_class,dtype = tf.int32)\n",
    "ConfM = sess.run(conf, feed_dict={x:newtest_X,y:newtest_Y})\n",
    "print (\"confusion matrix \\n\",ConfM)\n",
    "\n",
    "#Plotting the Confusion Matrix\n",
    "labels = ['Normal', 'Attack']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(ConfM)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
