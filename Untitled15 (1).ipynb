{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Started.....\n",
      "\n",
      "Original number of records in the training dataset before removing duplicates is:  494021\n",
      "Number of records in the training dataset after removing the duplicates is : 145586 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Started.....\n",
      "\n",
      "Original number of records in the testing dataset before removing duplicates is:  494021\n",
      "Number of records in the training dataset after removing the duplicates is : 145586 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8a4f4feb4c79>:104: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "0 Cost for this epoch is 1.2159499\n",
      "1 Cost for this epoch is 1.0835643\n",
      "2 Cost for this epoch is 0.9736317\n",
      "3 Cost for this epoch is 0.87325436\n",
      "4 Cost for this epoch is 0.78353786\n",
      "5 Cost for this epoch is 0.7093569\n",
      "6 Cost for this epoch is 0.6444284\n",
      "7 Cost for this epoch is 0.58739114\n",
      "8 Cost for this epoch is 0.5363549\n",
      "9 Cost for this epoch is 0.4904009\n",
      "Accuracy 0.19161183\n",
      "test Output is : [[  5.8746676   -4.915682  ]\n",
      " [ -0.49594003  -3.6017537 ]\n",
      " [  4.0938244   -8.801902  ]\n",
      " ...\n",
      " [  5.188534   -11.511798  ]\n",
      " [  5.9477673   -9.250149  ]\n",
      " [  4.9243584  -11.010562  ]]\n",
      "test labels are : [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "confusion matrix \n",
      " [[     0      0]\n",
      " [117690  27896]]\n",
      "new accuracy 0.19161183\n",
      "Accuracy calculated through confusion matrix 0.19161183080790736\n",
      "Precision \n",
      " 0.0\n",
      "Recall (DR)\n",
      " nan\n",
      "F1 Score is \n",
      " nan\n",
      "False Alarm Rate is \n",
      " 0.8083881691920927\n",
      "Efficincy is \n",
      " nan\n"
     ]
    }
   ],
   "source": [
    "#Importing all the required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "from tensorflow.contrib import rnn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Code Snippet remove the duplicates from the intrusion detection dataset\n",
    "print (\"Data Preprocessing Started.....\\n\")\n",
    "traindataframe = pd.read_csv(\"kddcup.data_10_percent\",header = None,engine = 'python',sep=\",\")\n",
    "recordcount = len(traindataframe)\n",
    "print (\"Original number of records in the training dataset before removing duplicates is: \" , recordcount)\n",
    "traindataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "newrecordcount = len(traindataframe)\n",
    "print (\"Number of records in the training dataset after removing the duplicates is :\", newrecordcount,\"\\n\")\n",
    "#Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "train_X = traindataframe.drop(traindataframe.columns[41],axis=1)\n",
    "labels = traindataframe.drop(traindataframe.columns[0:41],axis=1)\n",
    "\n",
    "\n",
    "# Convert Categorial data to the numerical data for the efficient classification\n",
    "train_X[train_X.columns[1:4]] = train_X[train_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "\n",
    "#labels.replace(['normal.',!'normal.'],[1,0],inplace = True)\n",
    "labels[labels[41]!='normal.'] = 0\n",
    "labels[labels[41]=='normal.'] = 1\n",
    "#print (labels[41].value_counts())\n",
    "\n",
    "inputX = train_X.loc[:,train_X.columns[0:41]].astype(float)\n",
    "\n",
    "labels.columns = [\"y1\"]\n",
    "#labels.loc[:,('y2')] = [0,1,1,1,0,1,0,0,0,]\n",
    "\n",
    "labels.loc[:,('y2')]=labels['y1'] ==0\n",
    "labels.loc[:,('y2')] = labels['y2'].astype(int)\n",
    "inputY = labels.as_matrix()\n",
    "inputX = inputX.as_matrix()\n",
    "\n",
    "\n",
    "# Code Snippet for test data\n",
    "print (\"Data Preprocessing Started.....\\n\")\n",
    "testdataframe = pd.read_csv(\"kddcup.data_10_percent\",header = None,engine = 'python',sep=\",\")\n",
    "testrecordcount = len(testdataframe)\n",
    "print (\"Original number of records in the testing dataset before removing duplicates is: \" , testrecordcount)\n",
    "testdataframe.drop_duplicates(subset=None, inplace=True)  # Python command to drop duplicates\n",
    "newtestrecordcount = len(testdataframe)\n",
    "print (\"Number of records in the training dataset after removing the duplicates is :\", newtestrecordcount,\"\\n\")\n",
    "#Dropping the labels to a different dataset which is used to train the recurrent neural network classifier\n",
    "test_X = testdataframe.drop(testdataframe.columns[41],axis=1)\n",
    "testlabels = testdataframe.drop(testdataframe.columns[0:41],axis=1)\n",
    "\n",
    "\n",
    "# Convert Categorial data to the numerical data for the efficient classification\n",
    "test_X[test_X.columns[1:4]] = test_X[test_X.columns[1:4]].stack().rank(method='dense').unstack()\n",
    "\n",
    "#labels.replace(['normal.',!'normal.'],[1,0],inplace = True)\n",
    "testlabels[testlabels!='normal'] = 0\n",
    "testlabels[testlabels=='normal'] = 1\n",
    "#print (testlabels[41].value_counts())\n",
    "\n",
    "inputX_test = test_X.loc[:,test_X.columns[0:41]].astype(float)\n",
    "\n",
    "testlabels.columns = [\"y1\"]\n",
    "\n",
    "testlabels.loc[:,('y2')]=testlabels['y1'] ==0\n",
    "testlabels.loc[:,('y2')] = testlabels['y2'].astype(int)\n",
    "inputY_test = testlabels.as_matrix()\n",
    "inputX_test = inputX_test.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "display_step =1\n",
    "num_layers = 1\n",
    "#Input Placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,shape = [None,41], name = \"x-input\")\n",
    "    y = tf.placeholder(tf.float32, shape = [None,2],name = \"y-input\")\n",
    "#Weights and Biases\n",
    "with tf.name_scope(\"weights\"):\n",
    "    W = tf.Variable(tf.random_normal([41,2]))\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    b = tf.Variable(tf.random_normal([2]))\n",
    "\n",
    "#Model\n",
    "with tf.name_scope(\"splitx\"):\n",
    "    newx = tf.split(x,1,0)\n",
    "with tf.name_scope(\"multicellconfig\"):\n",
    "    multicell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.GRUCell(41) for _ in range (num_layers)], state_is_tuple=True)\n",
    "    \n",
    "with tf.variable_scope('mygrucell'):\n",
    "    outputs,states = tf.contrib.rnn.static_rnn(multicell,newx,dtype=tf.float32, scope = None)\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    output = tf.add(tf.matmul(outputs[-1],W),b)\n",
    "\n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y,1))\n",
    "    cast = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(cast)\n",
    "#create summary for the cost and accuracy\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary_op = tf.summary.merge_all()\n",
    "logs_path = \"tmp/gru/test10\"\n",
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(logs_path, graph = tf.get_default_graph())\n",
    "        for i in range (training_epochs):\n",
    "            _,summary = sess.run([optimizer,summary_op], feed_dict = {x:inputX, y:inputY})\n",
    "            writer.add_summary(summary,i)\n",
    "    \n",
    "            if (i) % display_step == 0:\n",
    "                print (i,\"Cost for this epoch is\",sess.run(cost, feed_dict ={x :inputX,y:inputY}))\n",
    "        print (\"Accuracy\",accuracy.eval(feed_dict = {x:inputX_test,y:inputY_test}))\n",
    "        print (\"test Output is :\", sess.run(output,feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        print (\"test labels are :\", sess.run(y,feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        pred_class = sess.run(tf.argmax(output,1),feed_dict = {x:inputX_test,y:inputY_test})\n",
    "        labels_class = sess.run(tf.argmax(y,1),feed_dict = {x:inputX_test,y:inputY_test})\n",
    "        conf = tf.contrib.metrics.confusion_matrix(labels_class,pred_class,dtype = tf.int32)\n",
    "        print (\"confusion matrix \\n\", sess.run(conf, feed_dict={x:inputX_test, y:inputY_test}))\n",
    "        n = tf.cast(labels_class,tf.int64)\n",
    "        newaccuracy = tf.contrib.metrics.accuracy(pred_class,n)\n",
    "        print (\"new accuracy\", sess.run (newaccuracy, feed_dict = {x:inputX_test, y:inputY_test}))\n",
    "        TP = conf[0,0]\n",
    "        FN = conf [0,1]\n",
    "        FP = conf[1,0]\n",
    "        TN = conf[1,1]\n",
    "        AccConf = (TP+TN)/(TP+FP+TN+FN)\n",
    "        print (\"Accuracy calculated through confusion matrix\", sess.run (AccConf, feed_dict = {x:inputX_test,y:inputY_test}))\n",
    "        # Precision\n",
    "        Precision = TP/(TP+FP)\n",
    "        print (\"Precision \\n\",sess.run(Precision,feed_dict ={x:inputX_test, y:inputY_test}))\n",
    "        #Recall\n",
    "        Recall = TP/(TP+FN)\n",
    "        print (\"Recall (DR)\\n\", sess.run(Recall,feed_dict={x:inputX_test,y:inputY_test}))\n",
    "        #F score\n",
    "        FScore = 2*((Precision*Recall)/(Precision+Recall))\n",
    "        print (\"F1 Score is \\n\",sess.run(FScore,{x:inputX_test, y:inputY_test}))\n",
    "        #False Alarm Rate\n",
    "        FAR = FP/(FP+TN)\n",
    "        print (\"False Alarm Rate is \\n\",sess.run(FAR,feed_dict ={x:inputX_test,y:inputY_test}))\n",
    "        #Efficiency\n",
    "        Efficiency = Recall/FAR\n",
    "        print(\"Efficincy is \\n\",sess.run(Efficiency,feed_dict = {x:inputX_test, y:inputY_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
